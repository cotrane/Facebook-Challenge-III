{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "preprocess test set to extract words and generate dictionaries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import string\n",
      "import time\n",
      "from scipy.sparse import *\n",
      "from scipy.io import mmwrite, mmread\n",
      "import csv\n",
      "from bs4 import BeautifulSoup\n",
      "from nltk.tag import brill\n",
      "from taggerfunctions import *\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.corpus import wordnet\n",
      "from ast import literal_eval\n",
      "import sklearn as sk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "uselesssymbols = ['. ','\\n',\"'\",'\\\"','(',')',',',';',':','?','!','&','$']\n",
      "def tokenizeWords(entry):\n",
      "    entryselect = []\n",
      "    soup = BeautifulSoup(entry)\n",
      "    for tag in soup.find_all([\"pre\", \"code\", \"a\", \"img\"]):\n",
      "        tag.decompose()\n",
      "    entry = soup.get_text().encode('ascii', 'ignore')\n",
      "    for symbol in uselesssymbols:\n",
      "        entry = entry.replace(symbol, ' ')\n",
      "    entrytok = nltk.word_tokenize(entry)\n",
      "    entrytok = [w.lower() for w in entrytok]\n",
      "    return tag_pos(entrytok)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tag_pos(entrytok):\n",
      "    entryselect = []\n",
      "    entrytoktag = braubt_tagger.tag(entrytok)\n",
      "    for tok, tag in entrytoktag:\n",
      "        if tag not in ('VBP', 'CC', 'CD', 'RB', 'TO', 'VB', 'DT', 'IN', 'PRP', 'VBZ', 'WDT', '-NONE-'):\n",
      "            try:\n",
      "                tok_lemmatized = lemmatizer.lemmatize(tok, get_wordnet_pos(tag))\n",
      "            except:\n",
      "                tok_lemmatized = lemmatizer.lemmatize(tok)\n",
      "            entryselect.append(tok_lemmatized)\n",
      "    return entryselect"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from taggerfunctions import *\n",
      "\n",
      "braubt_tagger = braubt_Tagger()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getDict(fname):\n",
      "    dictWords = {}\n",
      "    with open(fname, 'r') as f:\n",
      "        reader = csv.reader(f)\n",
      "        dictWords = {rows[0]:literal_eval(rows[1]) for rows in reader}\n",
      "    return pd.Series(dictWords)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fname = 'dictKeys.csv'\n",
      "dictKeys = getDict(fname)\n",
      "fname = 'dictWordsBodyFull.csv'\n",
      "dictWordsBody = getDict(fname)\n",
      "fname = 'dictWordsTitleNew.csv'\n",
      "dictWordsTitle = getDict(fname)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reader = pd.read_csv(\"Test.csv\", chunksize=100000)\n",
      "IdTest = []\n",
      "for chunk in reader:\n",
      "    for idnum in chunk['Id']:\n",
      "        IdTest.append(idnum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for idx in range(10): \n",
      "    dictId = {qid:qidnum for qid,qidnum in zip(IdTest[idx*200000:(idx+1)*200000], range(200000))}\n",
      "    invdictId = {qidnum:qid for qid,qidnum in zip(IdTest[idx*200000:(idx+1)*200000], range(200000))}\n",
      "    dictId = pd.Series(dictId)\n",
      "    fname = \"dictIdTest_\" + str(idx*200000) + \"-\" + str((idx+1)*200000) + \".csv\"\n",
      "    dictId.to_csv(fname)\n",
      "    invdictId = pd.Series(invdictId)\n",
      "    fname = \"invdictIdTest_\" + str(idx*200000) + \"-\" + str((idx+1)*200000) + \".csv\"\n",
      "    invdictId.to_csv(fname)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictId = {qid:qidnum for qid,qidnum in zip(IdTest[2000000:], range(len(Id[2000000:])))}\n",
      "invdictId = {qidnum:qid for qid,qidnum in zip(IdTest[2000000:], range(len(Id[2000000:])))}\n",
      "dictId = pd.Series(dictId)\n",
      "fname = \"dictIdTest_2000000-2013337.csv\"\n",
      "dictId.to_csv(fname)\n",
      "invdictId = pd.Series(invdictId)\n",
      "fname = \"invdictIdTest_2000000-2013337.csv\"\n",
      "invdictId.to_csv(fname)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lemmatizer = WordNetLemmatizer()\n",
      "reader = pd.read_csv(\"Test.csv\", chunksize=100000)\n",
      "timeStart = time.time()\n",
      "count=1\n",
      "dictId = getDict(\"dictIdTest_0-200000.csv\")\n",
      "testQWordsBody_lil = lil_matrix( (len(dictId), len(dictWordsBody)) )\n",
      "testQWordsTitle_lil = lil_matrix( (len(dictId), len(dictWordsTitle)) )\n",
      "for chunk in reader:\n",
      "    for Id,title,body in zip(chunk['Id'],chunk['Title'],chunk['Body']):\n",
      "        titlewords = tokenizeWords(title)\n",
      "        settitle = set(titlewords)\n",
      "        iterWords = list(w for w in settitle if w in dictWordsTitle.keys())\n",
      "        for word in iterWords:\n",
      "            testQWordsTitle_lil[dictId[str(Id)],dictWordsTitle[word]] += titlewords.count(word)\n",
      "        bodywords = tokenizeWords(body)\n",
      "        setbody = set(bodywords)\n",
      "        iterWords = list(w for w in setbody if w in dictWordsBody.keys())\n",
      "        for word in iterWords:\n",
      "            testQWordsBody_lil[dictId[str(Id)],dictWordsBody[word]] += bodywords.count(word)\n",
      "        if count % 100000 == 0:\n",
      "            print(\"entry {0:d} finished\".format(count))\n",
      "            print(\"time for 100000 loops: {0:.0f}s\".format(time.time() - timeStart))\n",
      "            timeStart = time.time()\n",
      "        if count % 200000 == 0:\n",
      "            fname = \"testWordsQTitle_\" + str(count-200000) + \"-\" + str(count) + \".mtx\"\n",
      "            mmwrite(fname, testQWordsTitle_lil)\n",
      "            fname = \"testWordsQBody_\" + str(count-200000) + \"-\" + str(count) + \".mtx\"\n",
      "            mmwrite(fname, testQWordsBody_lil)\n",
      "            testQWordsBody_lil = lil_matrix( (200000, len(dictWordsBody)) )\n",
      "            testQWordsTitle_lil = lil_matrix( (200000, len(dictWordsTitle)) )\n",
      "            print(\"files saved\")\n",
      "            fname = \"dictIdTest_\" + str(count) + \"-\" + str(count+200000) + \".csv\"\n",
      "            dictId = getDict(fname)\n",
      "        count+=1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lemmatizer = WordNetLemmatizer()\n",
      "reader = pd.read_csv(\"Test.csv\", chunksize=100000)\n",
      "timeStart = time.time()\n",
      "count=1\n",
      "dictId = getDict(\"dictIdTest_2000000-2013337.csv\")\n",
      "testQWordsBody_lil = lil_matrix( (len(dictId), len(dictWordsBody)) )\n",
      "testQWordsTitle_lil = lil_matrix( (len(dictId), len(dictWordsTitle)) )\n",
      "for chunk in reader:\n",
      "    for Id,title,body in zip(chunk['Id'],chunk['Title'],chunk['Body']):\n",
      "        if count > 2000000:\n",
      "            if count == 2000001:\n",
      "                print(\"start of evaluation\")\n",
      "            titlewords = tokenizeWords(title)\n",
      "            settitle = set(titlewords)\n",
      "            iterWords = list(w for w in settitle if w in dictWordsTitle.keys())\n",
      "            for word in iterWords:\n",
      "                testQWordsTitle_lil[dictId[str(Id)],dictWordsTitle[word]] += titlewords.count(word)\n",
      "            bodywords = tokenizeWords(body)\n",
      "            setbody = set(bodywords)\n",
      "            iterWords = list(w for w in setbody if w in dictWordsBody.keys())\n",
      "            for word in iterWords:\n",
      "                testQWordsBody_lil[dictId[str(Id)],dictWordsBody[word]] += bodywords.count(word)\n",
      "        count+=1\n",
      "fname = \"testWordsQTitle_2000000-2013337.mtx\"\n",
      "mmwrite(fname, testQWordsTitle_lil)\n",
      "fname = \"testWordsQBody_2000000-2013337.mtx\"\n",
      "mmwrite(fname, testQWordsBody_lil)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "start of evaluation\n"
       ]
      }
     ],
     "prompt_number": 26
    }
   ],
   "metadata": {}
  }
 ]
}